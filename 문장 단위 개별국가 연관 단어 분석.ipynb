{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7332580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a5fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook\\target_data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook')\n",
    "DATA_PATH = os.getcwd() + r'\\target_data'\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a671da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_PATH, '*.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4dca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for source in files:\n",
    "    temp_df = pd.read_excel(source, engine='openpyxl')\n",
    "    all_dfs.append(temp_df)\n",
    "df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d65954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['등급'].isin([1,2,3,4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50cca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87af3cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>학교명</th>\n",
       "      <th>등급</th>\n",
       "      <th>지문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경희대</td>\n",
       "      <td>1</td>\n",
       "      <td>1)  가: 저는 장민입니다.\\n    나: 저는 한나 요한손입니다.\\n2)  가:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경희대</td>\n",
       "      <td>1</td>\n",
       "      <td>1) 가: 어느 나라 사람입니까\\n   나: 저는 러시아 사람입니다.\\n2) 가: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경희대</td>\n",
       "      <td>1</td>\n",
       "      <td>1) 제 친구는 친절해요.\\n2) 이 사람은 제 친구 한나예요. 한나는 정말 똑똑해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경희대</td>\n",
       "      <td>1</td>\n",
       "      <td>1) 가: 그게 뭐예요?\\n   나: 커피예요.\\n2) 가: 이게 한국어로 뭐예요?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경희대</td>\n",
       "      <td>1</td>\n",
       "      <td>1) 가: 서점이 어디에 있어요?\\n   나: 2층에 있어요.\\n2)  가: 커피숍...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   학교명  등급                                                 지문\n",
       "0  경희대   1  1)  가: 저는 장민입니다.\\n    나: 저는 한나 요한손입니다.\\n2)  가:...\n",
       "1  경희대   1  1) 가: 어느 나라 사람입니까\\n   나: 저는 러시아 사람입니다.\\n2) 가: ...\n",
       "2  경희대   1  1) 제 친구는 친절해요.\\n2) 이 사람은 제 친구 한나예요. 한나는 정말 똑똑해...\n",
       "3  경희대   1  1) 가: 그게 뭐예요?\\n   나: 커피예요.\\n2) 가: 이게 한국어로 뭐예요?...\n",
       "4  경희대   1  1) 가: 서점이 어디에 있어요?\\n   나: 2층에 있어요.\\n2)  가: 커피숍..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426b3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_df = df['지문']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40eb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 분리 : . ! ? 를 이용해서 간단히 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92f5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_df = [ cell.replace('\\n','') for cell in cell_df]\n",
    "cell_df = [ cell.replace('.','.$') for cell in cell_df]\n",
    "cell_df = [ cell.replace('!','!$') for cell in cell_df]\n",
    "cell_df = [ cell.replace('?','?$') for cell in cell_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca97c289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_in_cell = [cell.split('$') for cell in cell_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a932976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for cell in sentences_in_cell:\n",
    "    for sent in cell:\n",
    "        if len(sent) > 2:\n",
    "            all_sentences.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTagger\n",
    "from Utagger.bin.utagger_py import UTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4916062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook\\Utagger\\한국어교재지문\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook\\Utagger\\한국어교재지문')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81795971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python call utagger function\n",
      "C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook\\Utagger\\Hlxcfg.txt\n"
     ]
    }
   ],
   "source": [
    "dllpath = r'../bin/UTaggerR64.dll'\n",
    "cfgpath = r'../Hlxcfg.txt'\n",
    "\n",
    "rt = UTagger.Load_global(dllpath, cfgpath)\n",
    "ut = UTagger(0) # 0은 객체 고유번호. 0~99 지정 가능. 같은 번호로 여러번 생성하면 안됨. 한 스레드당 하나씩 지정 필요.\n",
    "rt = ut.new_ucma() #객체 생성. 객체가 있어야 유태거 이용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e47f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66b90b4f7c54b458e209f6caf6c91f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_utagging = [ut.tag_line(sent,3) for sent in tqdm(all_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e546008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.release_ucma() #객체 해제\n",
    "UTagger.Release_global() #사전 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b6077d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 어깨번호 정제하고자 할 때 사용, 어깨번호는 여러숫자로 구성될 수 있으므로 마지막 2개만 사용\n",
    "def cutSemanticNum(word, all=False): # all : True이면 의미숫자를 다 잘라버림, False이면 두글자만 남김\n",
    "    splitted = word.split('__')\n",
    "    \n",
    "    if len(splitted) == 1: # __ 구분자가 없는단어는 원 단어 그대로 반환\n",
    "        return word\n",
    "    \n",
    "    if all==True:\n",
    "        return splitted[0]\n",
    "    \n",
    "    out_word = splitted[0]+'__'+splitted[1][-2:]\n",
    "    return out_word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be884882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utagger 가 생성한 한 어절에서 + 단위로 형태소를 자른 후 / 단위로 어휘와 태그를 분석\n",
    "def utagging_to_simple_tagging(utg):\n",
    "    #outlier가 발견되어 제거함\n",
    "    utg = utg.replace('NNG\\n','NNG')\n",
    "    \n",
    "    temp_wordset = utg.split('+') # utagger 가 생성한 결과에서 + 단위로 잘라줌\n",
    "   \n",
    "    wordset = []\n",
    "    for word_tag in temp_wordset:\n",
    "        seplist = word_tag.split('/') #/ 로 잘라지지 않는 경우에 대한 예외처리\n",
    "        if len(seplist) <= 1 :\n",
    "            continue\n",
    "        else :\n",
    "            wordset.append(seplist)\n",
    "    if len(wordset) <= 0:\n",
    "        return None\n",
    "   \n",
    "    try:\n",
    "        first_word = wordset[0][0]\n",
    "        first_tag = wordset[0][1]\n",
    "    except:\n",
    "        print(\"예외처리 : \", wordset)\n",
    "    \n",
    "    #동사(VV), 형용사(VA)로 시작하면 바로 첫 단어만 내보냄\n",
    "    if first_tag.startswith('VV') or first_tag.startswith('VA'):\n",
    "        return (first_word, first_tag)\n",
    "    \n",
    "    if len(wordset) == 1: #한 단어인 경우 \n",
    "         #명사류, 부사, 한글자 이상의 외국어는 사용\n",
    "        if first_tag.startswith('NNP') or first_tag.startswith('NNG') or first_tag.startswith('MAG') or (first_tag.startswith('SL') and len(first_word)>1) :\n",
    "            #if word_tag[1].startswith('SL'):\n",
    "            #            print(word_tag[0])\n",
    "            return (first_word, first_tag)\n",
    "\n",
    "    else: #여러 단어인 경우 \n",
    "        if first_tag.startswith('NNP') or first_tag.startswith('NNG') or first_tag.startswith('SL') or first_tag.startswith('XPN'): #첫 단어가 다음과 같으면\n",
    "            output = []\n",
    "            \n",
    "            if first_tag.startswith('XPN'): #XPN+NNG 는 합쳐서 NNG로\n",
    "                second_word = wordset[1][0]\n",
    "                second_tag = wordset[1][1]\n",
    "                if second_tag.startswith('NNG'):\n",
    "                        output.append((cutSemanticNum(first_word,True)+second_word, second_tag))\n",
    "                        #print(output[0])\n",
    "                return output\n",
    "            \n",
    "            for word_tag in wordset: #XPN+NNG 로 반환해야함--> NNG로\n",
    "                if word_tag[1].startswith('NNP') or word_tag[1].startswith('NNG') or (word_tag[1].startswith('SL') and len(word_tag[0])>1): #명사류를 찾음\n",
    "                    #if word_tag[1].startswith('SL'): #지울 것\n",
    "                    #    print(word_tag[0])\n",
    "                    output.append((word_tag[0], word_tag[1]))\n",
    "            return output\n",
    "\n",
    "    return None\n",
    "\n",
    "#문장 전체를 받아서 (단어,태그) 튜플 리스트로 반환\n",
    "def remain_available_words(doc): \n",
    "    result = []\n",
    "    for utg in doc.split(' '):\n",
    "        simple_tagged = utagging_to_simple_tagging(utg)\n",
    "        if simple_tagged == None:\n",
    "            continue\n",
    "                \n",
    "        if str(type(simple_tagged)) ==\"<class 'tuple'>\":\n",
    "            result.append(simple_tagged)\n",
    "        else :\n",
    "            for word_tag in simple_tagged:\n",
    "                result.append(word_tag)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a521ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tagging = [remain_available_words(sent) for sent in sent_utagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f0b3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 수집\n",
    "os.chdir(r'C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook')\n",
    "DATA_PATH = os.getcwd()\n",
    "filename = DATA_PATH+r'\\불용어목록.xlsx'\n",
    "stopword_df = pd.read_excel(filename, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40d478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거 필터\n",
    "stopword = list(stopword_df['불용어'])\n",
    "def stopword_filter(cell):\n",
    "    output = []\n",
    "    for word, tag in cell:\n",
    "        if word not in stopword:\n",
    "            output.append((word,tag))\n",
    "    return output      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ffeb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "sent_tagging = [stopword_filter(cell) for cell in sent_tagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c874699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 태그 수 : 6\n"
     ]
    }
   ],
   "source": [
    "#태그:빈도 사전구성\n",
    "tag_freq_dic = {}\n",
    "for cell in sent_tagging:\n",
    "    for word, tag in cell:\n",
    "        tag_freq_dic[tag] = tag_freq_dic.get(tag, 0) + 1\n",
    "print(\"고유 태그 수 : %d\" % len(tag_freq_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b43dbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어:태그 사전구성\n",
    "word_tag_dic = {}\n",
    "for sent in sent_tagging:\n",
    "    for word, tag in sent:\n",
    "        word_tag_dic[word] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8948eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀내의 튜플 리스트에서 워드만 추출\n",
    "def word_selector(cell):\n",
    "    output = []\n",
    "    for word, tag in cell:\n",
    "        if word not in stopword:\n",
    "            output.append(word)\n",
    "    return output      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48c773a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_wordlist = [word_selector(cell) for cell in sent_tagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1469b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 수 :  33988\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "print(\"문장 수 : \", len(sent_wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "648d3d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 출현 수 : 187301\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for cell in sent_wordlist:\n",
    "    cnt += len(cell)\n",
    "print('단어 출현 수 :', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1d8d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 단어 수 : 13672\n"
     ]
    }
   ],
   "source": [
    "#단어:빈도 사전\n",
    "word_freq_dic = {}\n",
    "for cell in sent_wordlist:\n",
    "    for word in cell:\n",
    "        word_freq_dic[word] = word_freq_dic.get(word, 0) + 1\n",
    "print(\"고유 단어 수 : %d\" % len(word_freq_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcc9f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상위단어 정렬 [(5940, '있__01'), (2567, '보__01'), (1991, '사람'), (1915, '가__01'), (1672, '좋__01'), (1362, '같'), (1213, '없__01'), (1198, '한국__05'), (1157, '남자__02'), (1112, '여자__02'), (1100, '많'), (1049, '오__01'), (986, '먹__02'), (967, '친구__02'), (947, '많이'), (877, '잘__02'), (818, '일__01'), (800, '안__02'), (793, '남__02'), (751, '오늘'), (740, '알'), (739, '시간__04'), (704, '생각하'), (699, '좀__02'), (682, '더__01'), (665, '만들'), (630, '살__01'), (627, '지금__03'), (598, '정말__01'), (587, '듣__01'), (577, '말__01'), (569, '받__01'), (557, '요즘'), (537, '음식'), (526, '크__01'), (525, '집__01'), (506, '주__01'), (499, '학생'), (482, '좋아하'), (474, '같이'), (464, '이번__01'), (455, '안녕하'), (422, '생각__01'), (416, '만나'), (415, '문제__06'), (408, '들__01'), (399, '모르'), (398, '다음__01'), (392, '맞__01'), (391, '나오'), (380, '전__08'), (364, '영화__01'), (356, '너무__01'), (348, '다__03'), (344, '배우__01'), (342, '가지'), (337, '쓰__03'), (336, '학교'), (333, '사'), (330, '선생님'), (328, '힘들'), (323, '또'), (322, '회사__04'), (319, '자신__01'), (319, '이야기'), (317, '말하'), (311, '찾'), (300, '여행__02'), (299, '중요하__02'), (294, '처음'), (294, '앞'), (292, '가장__01'), (291, '드리__01'), (290, '아이__01'), (287, '생활'), (287, '사회자'), (286, '못__04'), (285, '필요하'), (285, '어렵'), (280, '모두__01'), (279, '사회__07'), (279, '문화__01'), (275, '재미있'), (275, '아주__01'), (268, '가족__01'), (265, '날__01'), (263, '느끼__02'), (262, '한국어'), (260, '마음__01'), (259, '사용하__03'), (259, '먼저'), (258, '경우__03'), (256, '꼭__03'), (251, '감사하__05'), (244, '자주__01'), (244, '다시__01'), (243, '진행자'), (243, '보내'), (243, '기자__05'), (242, '인터넷'), (241, '함께'), (240, '정도__11'), (239, '서울__01'), (237, '타__02'), (237, '직원__03'), (236, '남__05'), (232, '따르__01'), (229, '내__02'), (227, '나라__01'), (225, '남__04'), (224, '다양하__01'), (223, '방법'), (220, '이제__01'), (219, '운동__02'), (219, '쉽'), (219, '사실__04'), (219, '말씀'), (218, '주말__02'), (216, '시작하__01'), (216, '바로__02'), (209, '내일'), (208, '왜__02'), (206, '손님'), (206, '다르__01'), (205, '책__01'), (203, '사진__07'), (203, '고향__02'), (202, '수업__04'), (201, '어제__01'), (200, '나__01'), (196, '조금__01'), (196, '공부하'), (195, '교수__06'), (193, '괜찮'), (191, '돈__01'), (189, '이유__04'), (189, '맛있'), (188, '전화__07'), (185, '최근'), (185, '잘하'), (184, '식당'), (183, '읽'), (183, '걸리__01'), (181, '에릭'), (180, '준비하'), (180, '부모님'), (180, '모습__01'), (180, '관심__01'), (178, '나누'), (177, '노래__01'), (176, '입__01'), (176, '시험__03'), (176, '물건'), (176, '다니'), (175, '동안__01'), (172, '왕__04'), (172, '어머니__01'), (172, '도움'), (170, '세계__02'), (169, '외국인'), (168, '부르__01'), (168, '대학__01'), (167, '저녁'), (167, '얼마'), (164, '공부__01'), (162, '활동__02'), (162, '속__01'), (162, '남__01'), (162, '경제__04'), (161, '옷__01'), (161, '들어가__01'), (159, '교육'), (158, '정보__06'), (158, '고맙__01'), (155, '마시'), (155, '눈__01'), (154, '방__07'), (153, '직접'), (153, '공연__02'), (152, '계속__04'), (150, '특히'), (150, '일하'), (150, '음악__01'), (150, '안__01'), (149, '물__01'), (149, '끝나'), (148, '길__01'), (147, '커피'), (147, '이용하__01'), (147, '얼마나'), (147, '보통'), (147, '마이클'), (147, '날씨__01'), (146, '혼자__01'), (146, '빨리'), (144, '의미__02'), (143, '드라마'), (143, '건강__03'), (141, '지역__03'), (141, '결혼'), (140, '작__01'), (140, '바라__01'), (139, '인기__01'), (139, '유키'), (139, '쓰__01'), (139, '바쁘'), (138, '아프'), (138, '뉴스'), (137, '중국__01'), (137, '이야기하'), (137, '아침'), (137, '새롭'), (137, '기다리'), (135, '비__01'), (135, '고민'), (134, '몸__01'), (134, '광고__02'), (132, '또한'), (132, '기업__01'), (132, '그냥'), (131, '서로__01'), (131, '결과__02'), (130, '이상__05'), (130, '내리__01'), (129, '스트레스'), (129, '계시'), (128, '축제__01'), (128, '나타나'), (127, '지내__01'), (127, '유미'), (126, '혹시__01'), (126, '윤'), (126, '높'), (126, '그때'), (125, '의견__01'), (125, '버스__02'), (124, '열심히'), (124, '상황__02'), (124, '내용__02'), (124, '나가'), (122, '시대__02'), (122, '선물__03'), (122, '기분__01'), (121, '서__01'), (121, '물론__01'), (120, '지나'), (120, '아직__01'), (118, '할머니'), (118, '일어나'), (118, '걷__02'), (116, '환경__02'), (116, '성격__02'), (116, '변화'), (116, '말씀하'), (115, '알리'), (115, '소개하__01'), (114, '이름'), (114, '쉬__03'), (113, '삶'), (113, '량__05'), (112, '자리__01'), (112, '왕명__02'), (112, '얻__01'), (112, '상품__03'), (112, '방송__01'), (111, '현재__02'), (111, '하늘__01'), (111, '특별하'), (111, '찍__02'), (111, '늦'), (111, '계획__01'), (110, '꿈__01'), (109, '제품__02'), (109, '빠르'), (108, '프로그램'), (108, '지하철'), (108, '전문가'), (108, '엄마'), (108, '손__01'), (108, '소비자'), (108, '부산__02'), (107, '하루__01'), (107, '예쁘'), (107, '가방__01'), (107, '가깝'), (106, '직장__05'), (106, '즐기__01'), (106, '일본__02'), (106, '이해하__02'), (106, '오후__02'), (106, '넣'), (105, '하나'), (105, '보이__02'), (104, '능력__02'), (103, '준비'), (103, '경기__11'), (102, '차이'), (102, '사고__12'), (102, '비싸'), (102, '밥__01'), (102, '나이__01'), (101, '나쁘__01'), (100, '유명하__01'), (100, '요리__05'), (100, '못하'), (100, '돕'), (99, '조사__30'), (99, '젊'), (99, '반__07'), (99, '바다'), (99, '도와주'), (98, '선배'), (98, '머리__01'), (98, '근처'), (97, '역사__04'), (97, '놓__01'), (97, '노라'), (97, '길__02'), (97, '그리하'), (97, '관계__05'), (97, '갖__01'), (96, '제임스__99'), (96, '영어__02'), (96, '언제__01'), (96, '과학'), (95, '즐겁'), (95, '알아보'), (94, '옆'), (94, '여성__01'), (94, '비슷하__02'), (94, '모시'), (94, '맵'), (94, '동아리__02'), (94, '돌아가'), (94, '걱정'), (93, '아름답'), (93, '두__01'), (93, '노력하__01'), (92, '소리__01'), (91, '힘__01'), (91, '존__99'), (91, '전통__06'), (91, '얼굴__01'), (91, '어리__03'), (91, '세대__02'), (91, '미국__03'), (91, '공간__05'), (90, '페이'), (90, '취업'), (90, '세우__01'), (90, '병원__02'), (90, '바꾸'), (90, '기술__01'), (90, '가르치__01'), (89, '제일__04'), (89, '아르바이트'), (89, '방학'), (89, '마지막'), (89, '따뜻하'), (89, '결혼하'), (89, '가격__03'), (88, '인간__01'), (88, '옛날'), (88, '맛__01'), (88, '대부분'), (87, '히__88'), (87, '질문'), (87, '영향__04'), (87, '들어오'), (86, '앉'), (86, '문__05'), (85, '효과__01'), (85, '토요일'), (85, '취미__04'), (85, '짧'), (85, '주인__01'), (85, '말씀드리'), (85, '동물'), (85, '갑자기'), (84, '확인하'), (84, '호랑이'), (84, '필요'), (84, '사이__01'), (84, '사랑__01'), (84, '미래__02'), (84, '놀__01'), (84, '기회__03'), (84, '기숙사'), (84, '가능하'), (83, '앵커__01'), (83, '생일__02'), (83, '산__01'), (83, '별로__01'), (83, '떨어지'), (82, '참__01'), (82, '전화하__02'), (82, '아버지'), (82, '불편하__01'), (82, '모이__01'), (82, '뒤__01'), (81, '항상'), (80, '행사__01'), (80, '없이'), (80, '그리__02'), (79, '지아'), (79, '정하__03'), (79, '작품__01'), (79, '우선__02'), (79, '시장__04'), (79, '방식__01'), (79, '밤__01'), (79, '모양__02'), (78, '원하__02'), (78, '아들'), (78, '세상__01'), (78, '성민'), (78, '말__03'), (78, '다녀오'), (78, '과정__03'), (78, '가수__11'), (77, '지키__01'), (77, '분야'), (77, '부분__01'), (77, '모임__01'), (77, '대신__03'), (77, '구경하'), (76, '한국인'), (76, '편하'), (76, '올리__01'), (76, '선수__05'), (75, '우리나라'), (75, '오히려'), (75, '어서__01'), (75, '아주머니'), (75, '센터__02'), (75, '딸__01'), (75, '도서관'), (74, '직업'), (74, '줄이'), (74, '주__26'), (74, '점점__01'), (74, '오래__02'), (74, '예술'), (74, '선택하'), (74, '도시__03'), (74, '단어'), (73, '행복하'), (73, '컴퓨터'), (73, '자__01'), (73, '넘__01'), (73, '김__06'), (73, '금요일'), (73, '개인__02'), (73, 'SNS'), (72, '점심'), (72, '이루__01'), (72, '외국어'), (72, '밖'), (72, '미안하'), (72, '과거__03'), (72, '경험'), (71, '줄리앙'), (71, '언어__01'), (71, '아기__01'), (71, '마을__01'), (71, '낮'), (71, '그림__01'), (71, '그동안'), (70, '청소년'), (70, '제주도__01'), (70, '입'), (70, '쇼핑'), (70, '반갑'), (70, '믿'), (70, '물어보'), (70, '뜻'), (70, '국가__01'), (69, '제인'), (69, '잡__01'), (69, '예약하'), (69, '안나'), (69, '느낌'), (69, '국민'), (68, '치__02'), (68, '어울리'), (68, '실험'), (68, '명절__01'), (68, '면접'), (68, '리포터'), (68, '꽃__01'), (67, '지구__04'), (67, '주앙'), (67, '적__02'), (67, '서비스'), (67, '살펴보'), (67, '분위기'), (67, '봉사__03'), (67, '발표__01'), (67, '바람__01'), (67, '달라지'), (66, '파티'), (66, '죽__01'), (66, '주변__04'), (66, '죄송하'), (66, '역할'), (66, '성주__04'), (66, '상__02'), (66, '비용__03'), (66, '덥__01'), (66, '대표적'), (66, '기사__10'), (66, '가게'), (65, '행동'), (65, '참여하'), (65, '장소__05'), (65, '위__01'), (65, '발전__01'), (65, '미리__01'), (65, '대학생'), (65, '노력__01'), (65, '기간__07'), (65, '거의__01'), (64, '휴대'), (64, '학기__02'), (64, '장민'), (64, '장면__04'), (64, '은행__02'), (64, '예전__01'), (64, '열리__02'), (64, '여기'), (64, '시작되__01'), (64, '시민'), (64, '불__01'), (64, '박사__01'), (64, '대상__11'), (64, '긍정적'), (64, '게임'), (63, '여름__01'), (63, '아오이'), (63, '스마트'), (63, '모으'), (63, '매우__01'), (63, '대학교'), (63, '늘어나'), (63, '남편__01'), (63, '결국'), (62, '해결하'), (62, '열__02'), (62, '언'), (62, '심하'), (62, '뚜'), (62, '디자인'), (62, '나츠미'), (62, '나중__01'), (62, '결혼식'), (61, '진짜'), (61, '주민'), (61, '제도__01'), (61, '장소이'), (61, '약속'), (61, '노인__01'), (61, '겨울'), (60, '흐엉'), (60, '폰'), (60, '잠깐'), (60, '오전__02'), (60, '오랜만'), (60, '싸__05'), (60, '신청하__01'), (60, '궁금하__01'), (60, '걱정하'), (59, '특징'), (59, '주로__01'), (59, '인생__01'), (59, '아파트'), (59, '식사__03'), (59, '상우'), (59, '상담__01'), (59, '사랑하'), (59, '사라지'), (59, '발생하'), (59, '레오'), (59, '덕분'), (59, '다니엘__99'), (59, '고객__04'), (58, '한옥'), (58, '카드'), (58, '이사__14'), (58, '외국__02'), (58, '오__07'), (58, '설날'), (58, '부모__01'), (58, '벌써'), (58, '버리__01'), (58, '가치__06'), (57, '행복__02'), (57, '팔'), (57, '진나'), (57, '증가하__01'), (57, '주인공'), (57, '전하'), (57, '잠__01'), (57, '이후__02'), (57, '원인__02'), (57, '언니'), (57, '신문__10'), (57, '숙제__03'), (57, '선택'), (57, '많아지'), (57, '높아지'), (57, '고기__01'), (56, '할아버지'), (56, '의사__12'), (56, '유지하__02'), (56, '웃'), (56, '올해'), (56, '소설__03'), (56, '반면__02'), (56, '마리오'), (56, '리타'), (56, '달리__01'), (56, '늘__01'), (56, '감기__04'), (55, '활용하'), (55, '키우'), (55, '춥'), (55, '춤__01'), (55, '찾아보'), (55, '업무__02'), (55, '아무리'), (55, '수진'), (55, '속도__01'), (55, '부족하'), (55, '며칠'), (55, '구하__01'), (55, '건물__03'), (54, '형태'), (54, '현상__04'), (54, '표현'), (54, '텔레비전'), (54, '주문하__01'), (54, '조선__05'), (54, '정부__08'), (54, '바뀌'), (54, '누르__01'), (54, '국제__02'), (53, '차__06'), (53, '조심하__02'), (53, '제대로'), (53, '작년'), (53, '자르갈'), (53, '실제로'), (53, '설명하'), (53, '도착하__01'), (52, '친하'), (52, '체험'), (52, '일찍'), (52, '이상하'), (52, '우진'), (52, '오른쪽'), (52, '연극'), (52, '쓰레기'), (52, '실수__01'), (52, '면__05'), (52, '리사'), (52, '낳__01'), (51, '표__04'), (51, '자연__01'), (51, '없어지'), (51, '대회__02'), (51, '나무__01'), (51, '끝__01'), (51, '김치__01'), (51, '기온'), (51, '공원__03'), (51, '고민하'), (50, '현실__02'), (50, '재미__01'), (50, '잊__01'), (50, '인재__02'), (50, '이사하__01'), (50, '예약'), (50, '상대방__02'), (50, '사귀'), (50, '범죄'), (50, '번호__02'), (50, '백화점'), (50, '목소리'), (50, '뇌__03'), (50, '끌'), (50, '글'), (50, '갖추'), (50, '가정__06'), (49, '홈페이지'), (49, '점원__01'), (49, '적극적'), (49, '일요일'), (49, '인물'), (49, '올라가'), (49, '여가__03'), (49, '시키__01'), (49, '수영__02'), (49, '소비__05'), (49, '묻__03'), (48, '휴대폰'), (48, '화__06'), (48, '창의력'), (48, '제이슨'), (48, '전공__05'), (48, '올리비아'), (48, '아마__01'), (48, '아나운서'), (48, '술__01'), (48, '사업__04'), (48, '사건__01'), (48, '복잡하'), (48, '베트남'), (48, '문자__02'), (48, '멀__02'), (48, '대화__06'), (48, '당시__02'), (48, '다리__01'), (48, '기억__02'), (48, '그날'), (48, '교실'), (47, '태어나'), (47, '치료'), (47, '준이치'), (47, '자전거'), (47, '입장__04'), (47, '의식__03'), (47, '유__05'), (47, '약__07'), (47, '안내__01'), (47, '소식__04'), (47, '빙빙'), (47, '땅__01'), (47, '깨끗하'), (47, '기능__03'), (47, '구체적'), (47, '곧__01'), (47, '경쟁'), (46, '휴가__01'), (46, '피해__01'), (46, '특별히'), (46, '최고__02'), (46, '지영'), (46, '이웃'), (46, '이미__01'), (46, '유행__02'), (46, '올__02'), (46, '역__14'), (46, '에너지'), (46, '뜨겁'), (46, '관광__02'), (46, '과일__01'), (46, '고등학교'), (46, '결정하__01'), (46, '강하__01'), (46, '가끔'), (45, '호텔'), (45, '표현하'), (45, '평소'), (45, '편리하'), (45, '중심__01'), (45, '주제__04'), (45, '재료__01'), (45, '시__13'), (45, '순간__03'), (45, '빌리'), (45, '무료__01'), (45, '멋있'), (45, '등산'), (45, '단체__02'), (45, '그대로'), (44, '풀'), (44, '평화__02'), (44, '졸업하'), (44, '연락하__02'), (44, '연락__02'), (44, '신청__01'), (44, '식품__01'), (44, '살아가'), (44, '빵__01'), (44, '불고기'), (44, '바지__01'), (44, '막__01'), (44, '떡__01'), (44, '남성__01'), (44, '경찰__04'), (43, '한글__01'), (43, '피곤하'), (43, '축구__04'), (43, '자료__03'), (43, '일주일'), (43, '얘기'), (43, '아내__01'), (43, '실력__02'), (43, '스스로'), (43, '생활하'), (43, '사무실'), (43, '다이어트'), (43, '넓'), (43, '길이__01'), (43, '관련'), (43, '건강하__02'), (43, '개발'), (43, '값'), (42, '커피숍'), (42, '카밀라'), (42, '연휴__02'), (42, '시청자'), (42, '비빔밥'), (42, '부탁'), (42, '부장__07'), (42, '반대__03'), (42, '떠나'), (42, '대단하'), (42, '거리__01'), (41, '회식'), (41, '전쟁'), (41, '잘못'), (41, '요금__01'), (41, '오르'), (41, '어른__01'), (41, '신기하__01'), (41, '신경__04'), (41, '세계적'), (41, '선물하'), (41, '목__01'), (41, '매일'), (41, '떡볶이'), (41, 'TV'), (40, '한국말'), (40, '학원__02'), (40, '칭찬'), (40, '작가__01'), (40, '자유__03'), (40, '옮기'), (40, '얼__01'), (40, '쌓이'), (40, '시설__03'), (40, '스포츠'), (40, '새로'), (40, '봄__01'), (40, '맞__02'), (40, '동생__01'), (40, '더욱'), (40, '대통령'), (40, '고르__01'), (40, '감독__02'), (40, '가짜'), (39, '환자__03'), (39, '플라스틱'), (39, '폭력'), (39, '켈리'), (39, '지난주'), (39, '적성__05'), (39, '자르__01'), (39, '운동하'), (39, '영상__01'), (39, '심각하__02'), (39, '세실__04'), (39, '설명'), (39, '법__01'), (39, '방향__01'), (39, '발전하__01'), (39, '박물관'), (39, '맡__01'), (39, '대표'), (39, '관객'), (38, '훨씬'), (38, '팬__01'), (38, '키__01'), (38, '추석__01'), (38, '청계천'), (38, '직장인'), (38, '지난번'), (38, '잘되'), (38, '자원__02'), (38, '일부__02'), (38, '일반__02'), (38, '응답하'), (38, '위치__01'), (38, '웨이'), (38, '원래__01'), (38, '왼쪽'), (38, '역시__01'), (38, '여유'), (38, '싸__01'), (38, '식량__03'), (38, '발달하'), (38, '만약'), (38, '마트'), (38, '등장하__01'), (38, '돌아오'), (38, '닭'), (38, '놀라'), (38, '기__21'), (38, '교사__09'), (38, '겪'), (38, '게다가'), (37, '편지__02'), (37, '탈__01'), (37, '축하하'), (37, '지식__02'), (37, '준서'), (37, '주소__01'), (37, '인간관계'), (37, '이유미'), (37, '유학__04'), (37, '오빠'), (37, '쑤'), (37, '실시하__03'), (37, '시원하'), (37, '상태__01'), (37, '살리'), (37, '사장__15'), (37, '병__04'), (37, '미__14'), (37, '마케팅'), (37, '마치__02'), (37, '똑같'), (37, '내려오'), (37, '급하'), (37, '국수__01'), (37, '교__88'), (37, '관리__04'), (37, '개발하'), (36, '호칭__02'), (36, '해외'), (36, '해__01'), (36, '편안하__01'), (36, '지방__05'), (36, '제공하__02'), (36, '전화번호'), (36, '자유롭'), (36, '잃어버리'), (36, '유럽__02'), (36, '연습__03'), (36, '아래__01'), (36, '신입__03'), (36, '빠지__02'), (36, '비행기'), (36, '배__01'), (36, '로봇'), (36, '들리__03'), (36, '노래방'), (36, '나무꾼'), (35, '형__01'), (35, '팅'), (35, '차례__01'), (35, '정신__12'), (35, '전혀__01'), (35, '자연스럽'), (35, '이해__06'), (35, '이어지'), (35, '이루어지'), (35, '울__01'), (35, '어떡하'), (35, '양념'), (35, '알려지'), (35, '소중하'), (35, '사용__04'), (35, '변하'), (35, '목표'), (35, '명동'), (35, '당연하__01'), (35, '다치__01'), (35, '냉장고'), (35, '남녀'), (35, '권리'), (35, '경제적'), (35, '갈비__01'), (35, '갈등'), (35, '가볍'), (34, '화장실'), (34, '현장__03'), (34, '한복'), (34, '편의점'), (34, '추__02'), (34, '지나치'), (34, '지갑__03'), (34, '정책__02'), (34, '장점__02'), (34, '잠시'), (34, '잃'), (34, '인터뷰'), (34, '의하__01'), (34, '유민__01'), (34, '움직이'), (34, '연구__03'), (34, '성공하'), (34, '붙'), (34, '방문하'), (34, '박__08'), (34, '민수__99'), (34, '맞추__01'), (34, '맑__01'), (34, '답답하')]\n"
     ]
    }
   ],
   "source": [
    "word_freq = []\n",
    "for key, value in word_freq_dic.items():\n",
    "    word_freq.append((value, key))\n",
    "word_freq.sort(reverse=True)\n",
    "print(\"상위단어 정렬\", word_freq[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "053d0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 붙이기\n",
    "cell_wordjoin = [' '.join(cutSemanticNum(word) for word in cell) for cell in sent_wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78435118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#국가 목록\n",
    "country_list = ['중국','일본','미국','베트남','프랑스','영국','태국','러시아','호주','인도','독일','스페인','노르웨이','이탈리아','그리스','필리핀','몽골', '캐나다','브라질','콜롬비아','케냐','칠레','이집트','미얀마','투발루','이란','스웨덴','몰디브','라오스','파라과이','코스타리카','자메이카','예멘','아이티','스위스','부탄','말레이시아','네팔','폴란드','파나마','터키','탄자니아','키리바시','캄보디아','인도네시아','우즈베키스탄','에티오피아','바레인','멕시코','나이지리아','과테말라']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "165d7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2957ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 리스트에 대한 쌍 조합 튜플을 반환\n",
    "def get_pairs_from_list(lst):\n",
    "    pairs = []\n",
    "    unique = list(set(lst)) #리스트 내 중복 제거\n",
    "    lst_len = len(unique)\n",
    "    for i in range(lst_len):\n",
    "        if i==(lst_len-1):\n",
    "            break\n",
    "        subset = unique[i+1:]\n",
    "\n",
    "        for item in subset:\n",
    "            pairs.append((unique[i], item))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ebee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#국가명으로 연관단어 네트워크 분석\n",
    "def country_semantic_network(country):\n",
    "    #대상 셀 선정\n",
    "    target_cell_wordjoin = [cell for cell in cell_wordjoin if country in cell]\n",
    "    #단어 합치기\n",
    "    target_cell_wordlist = [cell.split(\" \") for cell in target_cell_wordjoin]\n",
    "    word_dic = {}\n",
    "    for cell in target_cell_wordlist:\n",
    "        for word in cell:\n",
    "            word_dic[word] = word_dic.get(word, 0) + 1\n",
    "    print(\"총 단어 수 : %d\" % len(word_dic))\n",
    "    word_freq = []\n",
    "    for key, value in word_dic.items():\n",
    "        word_freq.append((value, key))\n",
    "    word_freq.sort(reverse=True)\n",
    "    #네트워크 생성, gephi를 위한 source-target 생성\n",
    "    edges = [get_pairs_from_list(cell) for cell in target_cell_wordlist]\n",
    "    G = nx.Graph()\n",
    "    for comb in edges:\n",
    "        for source, dest in comb:\n",
    "            if G.has_edge(source, dest):\n",
    "                G[source][dest]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(source, dest, weight=1)\n",
    "    print(\"생성된 그래프 노드 수: %d, 엣지 수 : %d\" %(G.number_of_nodes(), G.number_of_edges()))\n",
    "    ####degree centrality 추출 및 소팅(시간 소요됨)\n",
    "    start = time.time()\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    print(\"네트워크 분석 완료 time : %.2f seconds\" % (time.time()-start))\n",
    "    #####degree 추출 및 소팅\n",
    "    degree = G.degree()\n",
    "    degree_freq = []\n",
    "    for key, value in degree:\n",
    "        degree_freq.append((value, key))\n",
    "    degree_freq.sort(reverse=True)\n",
    "    #print(\"상위 degree\", degree_freq[:100])\n",
    "    \n",
    "    #결과 데이터 생성\n",
    "    global country_df\n",
    "    for dfreq, word in degree_freq[:200] :\n",
    "        country_df = country_df.append({'품사':word_tag_dic[word],'단어':word,'출현빈도':word_dic[word],'연결단어수':dfreq,'연결중심성':degree_centrality[word],'매개중심성':betweenness_centrality[word],'분석범위':country,'출현빈도/품사빈도':word_freq_dic[word]/tag_freq_dic[word_tag_dic[word]]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed7a3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 중국 분석 시작 =====================\n",
      "총 단어 수 : 644\n",
      "생성된 그래프 노드 수: 644, 엣지 수 : 5382\n",
      "네트워크 분석 완료 time : 1.65 seconds\n",
      "================== 중국 분석 종료 =====================\n",
      "\n",
      "================== 일본 분석 시작 =====================\n",
      "총 단어 수 : 501\n",
      "생성된 그래프 노드 수: 501, 엣지 수 : 3959\n",
      "네트워크 분석 완료 time : 0.92 seconds\n",
      "================== 일본 분석 종료 =====================\n",
      "\n",
      "================== 미국 분석 시작 =====================\n",
      "총 단어 수 : 479\n",
      "생성된 그래프 노드 수: 479, 엣지 수 : 3678\n",
      "네트워크 분석 완료 time : 0.77 seconds\n",
      "================== 미국 분석 종료 =====================\n",
      "\n",
      "================== 베트남 분석 시작 =====================\n",
      "총 단어 수 : 136\n",
      "생성된 그래프 노드 수: 136, 엣지 수 : 554\n",
      "네트워크 분석 완료 time : 0.06 seconds\n",
      "================== 베트남 분석 종료 =====================\n",
      "\n",
      "================== 프랑스 분석 시작 =====================\n",
      "총 단어 수 : 162\n",
      "생성된 그래프 노드 수: 162, 엣지 수 : 881\n",
      "네트워크 분석 완료 time : 0.08 seconds\n",
      "================== 프랑스 분석 종료 =====================\n",
      "\n",
      "================== 영국 분석 시작 =====================\n",
      "총 단어 수 : 156\n",
      "생성된 그래프 노드 수: 156, 엣지 수 : 1092\n",
      "네트워크 분석 완료 time : 0.08 seconds\n",
      "================== 영국 분석 종료 =====================\n",
      "\n",
      "================== 태국 분석 시작 =====================\n",
      "총 단어 수 : 107\n",
      "생성된 그래프 노드 수: 107, 엣지 수 : 502\n",
      "네트워크 분석 완료 time : 0.03 seconds\n",
      "================== 태국 분석 종료 =====================\n",
      "\n",
      "================== 러시아 분석 시작 =====================\n",
      "총 단어 수 : 121\n",
      "생성된 그래프 노드 수: 121, 엣지 수 : 748\n",
      "네트워크 분석 완료 time : 0.05 seconds\n",
      "================== 러시아 분석 종료 =====================\n",
      "\n",
      "================== 호주 분석 시작 =====================\n",
      "총 단어 수 : 83\n",
      "생성된 그래프 노드 수: 83, 엣지 수 : 651\n",
      "네트워크 분석 완료 time : 0.03 seconds\n",
      "================== 호주 분석 종료 =====================\n",
      "\n",
      "================== 인도 분석 시작 =====================\n",
      "총 단어 수 : 183\n",
      "생성된 그래프 노드 수: 183, 엣지 수 : 1099\n",
      "네트워크 분석 완료 time : 0.10 seconds\n",
      "================== 인도 분석 종료 =====================\n",
      "\n",
      "================== 독일 분석 시작 =====================\n",
      "총 단어 수 : 112\n",
      "생성된 그래프 노드 수: 112, 엣지 수 : 725\n",
      "네트워크 분석 완료 time : 0.04 seconds\n",
      "================== 독일 분석 종료 =====================\n",
      "\n",
      "================== 스페인 분석 시작 =====================\n",
      "총 단어 수 : 110\n",
      "생성된 그래프 노드 수: 110, 엣지 수 : 683\n",
      "네트워크 분석 완료 time : 0.04 seconds\n",
      "================== 스페인 분석 종료 =====================\n",
      "\n",
      "================== 노르웨이 분석 시작 =====================\n",
      "총 단어 수 : 41\n",
      "생성된 그래프 노드 수: 41, 엣지 수 : 191\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 노르웨이 분석 종료 =====================\n",
      "\n",
      "================== 이탈리아 분석 시작 =====================\n",
      "총 단어 수 : 79\n",
      "생성된 그래프 노드 수: 79, 엣지 수 : 511\n",
      "네트워크 분석 완료 time : 0.02 seconds\n",
      "================== 이탈리아 분석 종료 =====================\n",
      "\n",
      "================== 그리스 분석 시작 =====================\n",
      "총 단어 수 : 99\n",
      "생성된 그래프 노드 수: 99, 엣지 수 : 749\n",
      "네트워크 분석 완료 time : 0.04 seconds\n",
      "================== 그리스 분석 종료 =====================\n",
      "\n",
      "================== 필리핀 분석 시작 =====================\n",
      "총 단어 수 : 48\n",
      "생성된 그래프 노드 수: 48, 엣지 수 : 227\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 필리핀 분석 종료 =====================\n",
      "\n",
      "================== 몽골 분석 시작 =====================\n",
      "총 단어 수 : 59\n",
      "생성된 그래프 노드 수: 59, 엣지 수 : 528\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 몽골 분석 종료 =====================\n",
      "\n",
      "================== 캐나다 분석 시작 =====================\n",
      "총 단어 수 : 45\n",
      "생성된 그래프 노드 수: 45, 엣지 수 : 295\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 캐나다 분석 종료 =====================\n",
      "\n",
      "================== 브라질 분석 시작 =====================\n",
      "총 단어 수 : 34\n",
      "생성된 그래프 노드 수: 34, 엣지 수 : 163\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 브라질 분석 종료 =====================\n",
      "\n",
      "================== 콜롬비아 분석 시작 =====================\n",
      "총 단어 수 : 41\n",
      "생성된 그래프 노드 수: 41, 엣지 수 : 232\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 콜롬비아 분석 종료 =====================\n",
      "\n",
      "================== 케냐 분석 시작 =====================\n",
      "총 단어 수 : 26\n",
      "생성된 그래프 노드 수: 26, 엣지 수 : 145\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 케냐 분석 종료 =====================\n",
      "\n",
      "================== 칠레 분석 시작 =====================\n",
      "총 단어 수 : 22\n",
      "생성된 그래프 노드 수: 22, 엣지 수 : 121\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 칠레 분석 종료 =====================\n",
      "\n",
      "================== 이집트 분석 시작 =====================\n",
      "총 단어 수 : 59\n",
      "생성된 그래프 노드 수: 59, 엣지 수 : 360\n",
      "네트워크 분석 완료 time : 0.01 seconds\n",
      "================== 이집트 분석 종료 =====================\n",
      "\n",
      "================== 미얀마 분석 시작 =====================\n",
      "총 단어 수 : 24\n",
      "생성된 그래프 노드 수: 24, 엣지 수 : 122\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 미얀마 분석 종료 =====================\n",
      "\n",
      "================== 투발루 분석 시작 =====================\n",
      "총 단어 수 : 16\n",
      "생성된 그래프 노드 수: 16, 엣지 수 : 49\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 투발루 분석 종료 =====================\n",
      "\n",
      "================== 이란 분석 시작 =====================\n",
      "총 단어 수 : 34\n",
      "생성된 그래프 노드 수: 34, 엣지 수 : 289\n",
      "네트워크 분석 완료 time : 0.02 seconds\n",
      "================== 이란 분석 종료 =====================\n",
      "\n",
      "================== 스웨덴 분석 시작 =====================\n",
      "총 단어 수 : 27\n",
      "생성된 그래프 노드 수: 27, 엣지 수 : 182\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 스웨덴 분석 종료 =====================\n",
      "\n",
      "================== 몰디브 분석 시작 =====================\n",
      "총 단어 수 : 20\n",
      "생성된 그래프 노드 수: 20, 엣지 수 : 103\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 몰디브 분석 종료 =====================\n",
      "\n",
      "================== 라오스 분석 시작 =====================\n",
      "총 단어 수 : 13\n",
      "생성된 그래프 노드 수: 13, 엣지 수 : 54\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 라오스 분석 종료 =====================\n",
      "\n",
      "================== 파라과이 분석 시작 =====================\n",
      "총 단어 수 : 20\n",
      "생성된 그래프 노드 수: 20, 엣지 수 : 102\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 파라과이 분석 종료 =====================\n",
      "\n",
      "================== 코스타리카 분석 시작 =====================\n",
      "총 단어 수 : 19\n",
      "생성된 그래프 노드 수: 19, 엣지 수 : 139\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 코스타리카 분석 종료 =====================\n",
      "\n",
      "================== 자메이카 분석 시작 =====================\n",
      "총 단어 수 : 17\n",
      "생성된 그래프 노드 수: 17, 엣지 수 : 72\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 자메이카 분석 종료 =====================\n",
      "\n",
      "================== 예멘 분석 시작 =====================\n",
      "총 단어 수 : 11\n",
      "생성된 그래프 노드 수: 11, 엣지 수 : 39\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 예멘 분석 종료 =====================\n",
      "\n",
      "================== 아이티 분석 시작 =====================\n",
      "총 단어 수 : 14\n",
      "생성된 그래프 노드 수: 14, 엣지 수 : 64\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 아이티 분석 종료 =====================\n",
      "\n",
      "================== 스위스 분석 시작 =====================\n",
      "총 단어 수 : 11\n",
      "생성된 그래프 노드 수: 11, 엣지 수 : 43\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 스위스 분석 종료 =====================\n",
      "\n",
      "================== 부탄 분석 시작 =====================\n",
      "총 단어 수 : 12\n",
      "생성된 그래프 노드 수: 12, 엣지 수 : 39\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 부탄 분석 종료 =====================\n",
      "\n",
      "================== 말레이시아 분석 시작 =====================\n",
      "총 단어 수 : 6\n",
      "생성된 그래프 노드 수: 6, 엣지 수 : 15\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 말레이시아 분석 종료 =====================\n",
      "\n",
      "================== 네팔 분석 시작 =====================\n",
      "총 단어 수 : 26\n",
      "생성된 그래프 노드 수: 26, 엣지 수 : 181\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 네팔 분석 종료 =====================\n",
      "\n",
      "================== 폴란드 분석 시작 =====================\n",
      "총 단어 수 : 8\n",
      "생성된 그래프 노드 수: 8, 엣지 수 : 28\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 폴란드 분석 종료 =====================\n",
      "\n",
      "================== 파나마 분석 시작 =====================\n",
      "총 단어 수 : 3\n",
      "생성된 그래프 노드 수: 3, 엣지 수 : 3\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 파나마 분석 종료 =====================\n",
      "\n",
      "================== 터키 분석 시작 =====================\n",
      "총 단어 수 : 8\n",
      "생성된 그래프 노드 수: 8, 엣지 수 : 28\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 터키 분석 종료 =====================\n",
      "\n",
      "================== 탄자니아 분석 시작 =====================\n",
      "총 단어 수 : 9\n",
      "생성된 그래프 노드 수: 9, 엣지 수 : 36\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 탄자니아 분석 종료 =====================\n",
      "\n",
      "================== 키리바시 분석 시작 =====================\n",
      "총 단어 수 : 2\n",
      "생성된 그래프 노드 수: 2, 엣지 수 : 1\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 키리바시 분석 종료 =====================\n",
      "\n",
      "================== 캄보디아 분석 시작 =====================\n",
      "총 단어 수 : 17\n",
      "생성된 그래프 노드 수: 17, 엣지 수 : 136\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 캄보디아 분석 종료 =====================\n",
      "\n",
      "================== 인도네시아 분석 시작 =====================\n",
      "총 단어 수 : 0\n",
      "생성된 그래프 노드 수: 0, 엣지 수 : 0\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 인도네시아 분석 종료 =====================\n",
      "\n",
      "================== 우즈베키스탄 분석 시작 =====================\n",
      "총 단어 수 : 2\n",
      "생성된 그래프 노드 수: 2, 엣지 수 : 1\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 우즈베키스탄 분석 종료 =====================\n",
      "\n",
      "================== 에티오피아 분석 시작 =====================\n",
      "총 단어 수 : 9\n",
      "생성된 그래프 노드 수: 9, 엣지 수 : 36\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 에티오피아 분석 종료 =====================\n",
      "\n",
      "================== 바레인 분석 시작 =====================\n",
      "총 단어 수 : 11\n",
      "생성된 그래프 노드 수: 11, 엣지 수 : 55\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 바레인 분석 종료 =====================\n",
      "\n",
      "================== 멕시코 분석 시작 =====================\n",
      "총 단어 수 : 0\n",
      "생성된 그래프 노드 수: 0, 엣지 수 : 0\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 멕시코 분석 종료 =====================\n",
      "\n",
      "================== 나이지리아 분석 시작 =====================\n",
      "총 단어 수 : 19\n",
      "생성된 그래프 노드 수: 19, 엣지 수 : 171\n",
      "네트워크 분석 완료 time : 0.00 seconds\n",
      "================== 나이지리아 분석 종료 =====================\n",
      "\n",
      "================== 과테말라 분석 시작 =====================\n",
      "총 단어 수 : 9\n",
      "생성된 그래프 노드 수: 9, 엣지 수 : 36\n",
      "네트워크 분석 완료 time : 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 과테말라 분석 종료 =====================\n",
      "\n",
      "C:\\Users\\Kislee\\PycharmProjects\\Korean_textbook\\save_data\\국가별 연관단어 분석.xlsx 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "#결과 데이터 생성\n",
    "country_df = pd.DataFrame()\n",
    "country_df = pd.DataFrame(columns=['품사','단어','출현빈도','출현빈도/품사빈도','연결단어수','연결중심성','매개중심성','분석범위'])\n",
    "\n",
    "for country in country_list:\n",
    "    print(\"================== %s 분석 시작 =====================\" % country)\n",
    "    country_semantic_network(country)\n",
    "    print(\"================== %s 분석 종료 =====================\" % country)\n",
    "    print()\n",
    "    \n",
    "DATA_PATH = os.getcwd() + r'\\save_data'\n",
    "filename = DATA_PATH+'\\국가별 연관단어 분석.xlsx'\n",
    "country_df.to_excel(filename)\n",
    "print(\"%s 저장 완료!\" %filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1434caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 끝."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
